#!/usr/bin/env python3
"""Lean Kernel Arena - Tool for managing Lean kernel tests and checkers."""

import argparse
import datetime
import fnmatch
import json
import os
import resource
import shutil
import subprocess
import sys
import tempfile
import time
from pathlib import Path

import yaml
import jsonschema
import markdown
from jinja2 import Environment, FileSystemLoader, select_autoescape
import shlex

# Global verbose flag
VERBOSE = False

# Timing/measurement utilities


def get_project_root() -> Path:
    """Get the project root directory."""
    return Path(__file__).parent.resolve()


def load_schema(schema_name: str) -> dict:
    """Load a JSON schema file."""
    schema_file = get_project_root() / "schemas" / f"{schema_name}.json"
    if not schema_file.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_file}")
    with open(schema_file, "r") as f:
        return json.load(f)


def validate_yaml_data(data: dict, schema_name: str, file_path: Path) -> None:
    """Validate YAML data against a schema."""
    try:
        schema = load_schema(schema_name)
        jsonschema.validate(data, schema)
    except jsonschema.ValidationError as e:
        # Format a helpful error message
        error_path = " -> ".join(str(p) for p in e.absolute_path) if e.absolute_path else "root"
        print(f"Schema validation error in {file_path}:")
        print(f"  Path: {error_path}")
        print(f"  Error: {e.message}")
        if e.validator_value:
            print(f"  Expected: {e.validator_value}")
        if hasattr(e, 'instance') and e.instance is not None:
            print(f"  Found: {e.instance}")
        sys.exit(1)
    except FileNotFoundError as e:
        print(f"Error: {e}")
        sys.exit(1)


def format_duration(seconds: float) -> str:
    """Format a duration in seconds to a human-readable string."""
    if seconds >= 3600:
        return f"{seconds / 3600:.1f}\u202fh"
    elif seconds >= 60:
        return f"{seconds / 60:.1f}\u202fm"
    elif seconds >= 1:
        return f"{seconds:.1f}\u202fs"
    else:
        return f"{seconds * 1000:.0f}\u202fms"


def format_memory(bytes_value: float) -> str:
    """Format memory usage in bytes to a human-readable string."""
    if bytes_value >= 1024 * 1024 * 1024:
        return f"{bytes_value / (1024 * 1024 * 1024):.1f}\u202fGB"
    elif bytes_value >= 1024 * 1024:
        return f"{bytes_value / (1024 * 1024):.1f}\u202fMB"
    elif bytes_value >= 1024:
        return f"{bytes_value / 1024:.1f}\u202fKB"
    else:
        return f"{bytes_value:.0f}\u202fB"


def format_unitless(count: int) -> str:
    """Format unitless count to a human-readable string with SI prefixes."""
    if count >= 1_000_000_000_000:
        return f"{count / 1_000_000_000_000:.1f}\u202fT"
    elif count >= 1_000_000_000:
        return f"{count / 1_000_000_000:.1f}\u202fG"
    elif count >= 1_000_000:
        return f"{count / 1_000_000:.1f}\u202fM"
    elif count >= 1_000:
        return f"{count / 1_000:.1f}\u202fk"
    else:
        return str(count)


def format_instructions(instruction_count: int) -> str:
    """Format instruction count to a human-readable string with SI prefixes."""
    if instruction_count >= 1_000_000_000:
        return f"{instruction_count / 1_000_000_000:.1f}\u202fG"
    elif instruction_count >= 1_000_000:
        return f"{instruction_count / 1_000_000:.1f}\u202fM"
    elif instruction_count >= 1_000:
        return f"{instruction_count / 1_000:.1f}\u202fk"
    else:
        return str(instruction_count)


def convert_instructions_to_time(instructions: int, instructions_per_second: float) -> float:
    """Convert instruction count to equivalent time in seconds."""
    if instructions_per_second > 0 and instructions > 0:
        return instructions / instructions_per_second
    return 0.0


def render_markdown(text: str) -> str:
    """Render markdown text to HTML."""
    if not text:
        return ""
    
    # Configure markdown with extensions for better HTML output
    md = markdown.Markdown(extensions=['extra', 'codehilite', 'toc'])
    return md.convert(text.strip())


def measure_perf_with_fallback(
    cmd: str | list[str],
    cwd: Path | None = None,
    env: dict | None = None,
    shell: bool = False,
    capture_output: bool = True,
) -> tuple[subprocess.CompletedProcess, dict]:
    """Run a command and measure performance metrics using perf + GNU time, with fallback.
    
    Returns:
        Tuple of (subprocess result, metrics dict)
        
    Metrics dict contains:
    - wall_time: Wall clock time in seconds
    - cpu_time: CPU time in seconds (measured via perf task-clock)
    - max_rss: Maximum RSS in bytes (measured via GNU time)
    - instructions: Instruction count (measured via perf)
    """
    # Record wall time manually as fallback
    start_wall_time = time.time()
    
    # Try to use perf if available
    metrics = {}
    use_perf = False
    
    # Check if perf is available
    try:
        perf_check = subprocess.run(
            ["perf", "--version"], 
            capture_output=True, 
            timeout=2
        )
        if perf_check.returncode == 0:
            use_perf = True
    except (FileNotFoundError, subprocess.TimeoutExpired):
        use_perf = False
    
    if use_perf:
        # Use perf + GNU time for comprehensive measurements
        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=".perf") as perf_tmp:
            perf_tmp_path = perf_tmp.name
        
        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=".time") as time_tmp:
            time_tmp_path = time_tmp.name
        
        try:
            # Build nested command: perf stat ... -- time -f "..." -o tmpfile -- original_command
            perf_cmd = [
                "perf", "stat", "-j", "-o", perf_tmp_path,
                "-e", "duration_time",  # wall-clock time
                "-e", "task-clock",     # cpu time
                "-e", "instructions",   # instruction count
                "--"
            ]
            
            # Add GNU time wrapper for max RSS measurement
            time_fmt = "real_seconds=%e\nuser_seconds=%U\nsys_seconds=%S\nmax_rss_kb=%M"
            time_cmd = ["time", "-f", time_fmt, "-o", time_tmp_path, "--"]
            
            if isinstance(cmd, list):
                full_cmd = perf_cmd + time_cmd + cmd
            else:
                if shell:
                    full_cmd = perf_cmd + time_cmd + ["sh", "-c", cmd]
                else:
                    full_cmd = perf_cmd + time_cmd + cmd.split()
            
            # Set up environment
            perf_env = (env or os.environ).copy()
            perf_env["LC_ALL"] = "C"  # Ensure perf outputs valid JSON
            
            # Run with nested perf + time
            result = subprocess.run(
                full_cmd,
                cwd=cwd,
                env=perf_env,
                shell=False,
                capture_output=capture_output,
                text=True,
            )
            
            # Parse perf output
            perf_metrics = {}
            try:
                with open(perf_tmp_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                data = json.loads(line)
                                if "event" in data and "counter-value" in data:
                                    event = data["event"]
                                    value = float(data["counter-value"])
                                    unit = data.get("unit", "")
                                    
                                    if event in ["duration_time", "task-clock"]:
                                        # Time events are in nanoseconds by default
                                        if unit == "msec":
                                            value *= 1e-3
                                        elif unit == "ns" or unit == "":
                                            value *= 1e-9  # Convert nanoseconds to seconds
                                        perf_metrics[event] = value
                                    elif event == "instructions":
                                        # Instructions are count (no unit conversion needed)
                                        perf_metrics[event] = int(value)
                            except (json.JSONDecodeError, ValueError, KeyError):
                                continue
            except Exception:
                pass
            
            # Parse GNU time output
            time_metrics = {}
            try:
                with open(time_tmp_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        if '=' in line:
                            k, v = line.split('=', 1)
                            time_metrics[k.strip()] = v.strip()
            except Exception:
                pass
            
            # Clean up temporary files
            try:
                os.unlink(perf_tmp_path)
                os.unlink(time_tmp_path)
            except:
                pass
            
            # Extract metrics with fallbacks
            metrics["wall_time"] = perf_metrics.get("duration_time", time.time() - start_wall_time)
            metrics["cpu_time"] = perf_metrics.get("task-clock", 0.0)
            metrics["instructions"] = perf_metrics.get("instructions", 0)
            
            # Extract max RSS from GNU time
            try:
                max_rss_kb = int(time_metrics.get("max_rss_kb", "0"))
                metrics["max_rss"] = max_rss_kb * 1024  # Convert KB to bytes
            except Exception:
                metrics["max_rss"] = 0
        
        except Exception:
            # If perf+time fails completely, fall back to basic measurement
            use_perf = False
    
    if not use_perf:
        # Final fallback: run normally and measure wall time only
        result = subprocess.run(
            cmd,
            cwd=cwd,
            env=env,
            shell=shell,
            capture_output=capture_output,
            text=True,
        )
        metrics["wall_time"] = time.time() - start_wall_time
        metrics["cpu_time"] = 0.0  # Not measured here
        metrics["max_rss"] = 0.0
        metrics["instructions"] = 0
    
    return result, metrics


def run_cmd(
    cmd: str | list[str],
    cwd: Path | None = None,
    env: dict | None = None,
    shell: bool = False,
    capture_output: bool = True,
    measure_perf: bool = False,
) -> subprocess.CompletedProcess:
    """Run a command with optional verbose output and performance measurement.
    
    Args:
        cmd: Command to run (string for shell=True, list for shell=False)
        cwd: Working directory
        env: Environment variables
        shell: Whether to run as shell command
        capture_output: Whether to capture stdout/stderr
        measure_perf: Whether to measure detailed performance metrics
    
    Returns:
        CompletedProcess instance with additional attributes:
        - wall_time: Wall clock time in seconds
        - cpu_time: CPU time in seconds (if available)
        - max_rss: Maximum RSS in bytes (if available)
        - instructions: Instruction count (if available)
    """
    global VERBOSE
    
    # Format command for display
    if isinstance(cmd, list):
        cmd_str = " ".join(cmd)
    else:
        cmd_str = cmd
    
    if VERBOSE:
        cwd_str = f" (in {cwd})" if cwd else ""
        print(f"    $ {cmd_str}{cwd_str}")
    
    if measure_perf:
        # Use detailed performance measurement
        result, metrics = measure_perf_with_fallback(
            cmd, cwd=cwd, env=env, shell=shell, capture_output=capture_output
        )
        
        # Add metrics as attributes to the result
        result.wall_time = metrics["wall_time"]
        result.cpu_time = metrics["cpu_time"]
        result.max_rss = metrics["max_rss"]
        result.instructions = metrics.get("instructions", 0)
        
        if VERBOSE:
            status = "ok" if result.returncode == 0 else f"FAILED (exit {result.returncode})"
            metrics_str = f"wall: {format_duration(result.wall_time)}"
            # Show real CPU time from perf (task-clock), not converted from instructions
            if result.cpu_time > 0:
                metrics_str += f", cpu: {format_duration(result.cpu_time)}"
            metrics_str += f", rss: {format_memory(result.max_rss)}"
            # Show instruction count separately
            if result.instructions > 0:
                metrics_str += f", inst: {result.instructions:,}"
            print(f"      -> {status} ({metrics_str})")
    else:
        # Use simple timing
        start_time = time.time()
        
        result = subprocess.run(
            cmd,
            cwd=cwd,
            env=env,
            shell=shell,
            capture_output=capture_output,
            text=True,
        )
        
        elapsed = time.time() - start_time
        
        # Add basic timing as attributes
        result.wall_time = elapsed
        result.cpu_time = 0.0  # Not measured
        result.max_rss = 0.0   # Not measured
        result.instructions = 0  # Not measured
        
        if VERBOSE:
            status = "ok" if result.returncode == 0 else f"FAILED (exit {result.returncode})"
            print(f"      -> {status} in {format_duration(elapsed)}")
    
    return result


def get_lean_toolchain(directory: Path) -> str | None:
    """Read the lean-toolchain file from a directory and return the toolchain string.
    
    Args:
        directory: Directory to look for lean-toolchain file
        
    Returns:
        Toolchain string (e.g., 'leanprover/lean4:v4.27.0-rc1'), or None if not found
    """
    toolchain_file = directory / "lean-toolchain"
    if toolchain_file.exists():
        try:
            with open(toolchain_file, "r") as f:
                return f.read().strip()
        except Exception as e:
            print(f"  Warning: Could not read {toolchain_file}: {e}")
    return None


def run_lean4export(lean4export_dir: Path, module_name: str, export_decls: list | None, cwd: Path, out_file: Path) -> bool:
    """Run lean4export (via lake env) to export a module.

    lean4export_dir: path to the checked-out/build lean4export repo
    module_name: module name to export
    export_decls: optional list of declaration names to pass after --
    cwd: working directory to run lake env from
    out_file: output path for NDJSON
    """
    lean4export_bin = lean4export_dir / ".lake" / "build" / "bin" / "lean4export"
    if not lean4export_bin.exists():
        print(f"  Error: lean4export binary not found at {lean4export_bin}")
        return False

    # Build command
    cmd = f"lake env {lean4export_bin} {module_name}"
    if export_decls:
        # Quote each decl for shell safety
        decls = " ".join(shlex.quote(str(d)) for d in export_decls)
        cmd += f" -- {decls}"
    cmd += f" > {out_file}"

    result = run_cmd(cmd, cwd=cwd, shell=True)
    if result.returncode != 0:
        print(f"  Export failed: {result.stderr}")
        return False
    return True


def setup_lean4export(toolchain: str) -> Path | None:
    """Clone and build lean4export for a specific Lean toolchain.
    
    Args:
        toolchain: Lean toolchain string (e.g., 'leanprover/lean4:v4.27.0-rc1')
        
    Returns:
        Path to lean4export directory for this toolchain, or None on failure
        
    Note: Failed temporary directories are left in place for debugging purposes.
    """
    # Sanitize toolchain string for use in file paths
    toolchain_dir_name = toolchain.replace("/", "_").replace(":", "_")
    build_base_dir = get_project_root() / "_build"
    lean4export_dir = build_base_dir / "lean4export" / toolchain_dir_name
    
    if not lean4export_dir.exists():
        print(f"  Cloning lean4export for toolchain {toolchain}...")
        
        # Work in temporary directory first
        lean4export_tmp_dir = Path(str(lean4export_dir) + ".tmp")
        
        # Clean up any existing temporary directory
        if lean4export_tmp_dir.exists():
            shutil.rmtree(lean4export_tmp_dir)
        
        lean4export_tmp_dir.mkdir(parents=True, exist_ok=True)
        
        clone_cmd = ["git", "clone", "--branch", "arena_json_output",
                    "https://github.com/leanprover/lean4export",
                    str(lean4export_tmp_dir)]
        result = run_cmd(clone_cmd)
        if result.returncode != 0:
            print(f"  Error cloning lean4export: {result.stderr}")
            return None

        # Set the specific toolchain
        toolchain_file = lean4export_tmp_dir / "lean-toolchain"
        try:
            with open(toolchain_file, "w") as f:
                f.write(toolchain + "\n")
        except Exception as e:
            print(f"  Error writing lean-toolchain file: {e}")
            return None

        print(f"  Building lean4export with toolchain {toolchain}...")
        result = run_cmd("lake build", cwd=lean4export_tmp_dir, shell=True)
        if result.returncode != 0:
            print(f"  Error building lean4export: {result.stderr}")
            return None
        
        # Move temporary directory to final location atomically
        try:
            lean4export_tmp_dir.rename(lean4export_dir)
        except Exception as e:
            print(f"  Error moving lean4export directory to final location: {e}")
            return None
    
    return lean4export_dir


def load_yaml_files(directory: Path, schema_name: str) -> list[dict]:
    """Load all YAML files from a directory with schema validation."""
    items = []
    if not directory.exists():
        return items
    # Sort files alphabetically to avoid dependency on filesystem order
    for file in sorted(directory.glob("*.yaml")):
        with open(file, "r") as f:
            data = yaml.safe_load(f)
            
            # Validate against schema before processing
            validate_yaml_data(data, schema_name, file)
            
            data["_file"] = file.name
            # Derive name from filename (without .yaml extension)
            data["name"] = file.stem
            items.append(data)
    return items


def load_test_descriptions() -> list[dict]:
    """Load test YAML definitions for accessing descriptions and configuration."""
    return load_yaml_files(get_project_root() / "tests", "test")


def load_checkers() -> list[dict]:
    """Load all checker definitions."""
    return load_yaml_files(get_project_root() / "checkers", "checker")


def find_test_by_name(name: str) -> dict | None:
    """Find a test by name (including expanded subtests)."""
    results = find_items_by_pattern(name, "tests")
    return results[0] if results else None


def find_checker_by_name(name: str) -> dict | None:
    """Find a checker by name."""
    results = find_items_by_pattern(name, "checkers")
    return results[0] if results else None


def find_items_by_pattern(pattern: str, item_type: str) -> list[dict]:
    """Find tests or checkers by glob pattern.
    
    Args:
        pattern: Name or glob pattern to match against
        item_type: "tests" or "checkers"
    
    Returns:
        List of matching items (tests or checkers)
    """
    if item_type == "tests":
        items = load_tests()
    elif item_type == "checkers":
        items = load_checkers()
    else:
        raise ValueError(f"Invalid item_type: {item_type}")
    
    # If pattern contains glob characters, use glob matching
    if any(char in pattern for char in ['*', '?', '[', ']']):
        return [item for item in items if fnmatch.fnmatch(item["name"], pattern)]
    else:
        # Exact match
        return [item for item in items if item["name"] == pattern]


# =============================================================================
# Source setup - shared by tests and checkers
# =============================================================================


def setup_source_directory(
    config: dict, 
    base_dir: Path, 
    local_base_path: Path | None = None,
) -> Path | None:
    """Set up a source directory for tests or checkers.
    
    Handles four cases:
    - url: Clone a git repository
    - dir: Use a local directory
    - leanfile: Set up a standalone Test module from a Lean file
    - neither: Create an empty directory
    
    Args:
        config: Test or checker configuration dict with name, url, dir, ref, rev, leanfile
        base_dir: Base directory where the work/build directory should be created
        local_base_path: Base path for local directories (defaults to project root)
    
    Returns the working directory path, or None on failure.
    """
    name = config["name"]
    url = config.get("url")
    local_dir = config.get("dir")
    lean_file_path = config.get("leanfile")
    ref = config.get("ref")
    rev = config.get("rev")

    if local_base_path is None:
        local_base_path = get_project_root()

    work_dir = base_dir / name
    
    # Clean up existing work directory
    if work_dir.exists():
        shutil.rmtree(work_dir)
    work_dir.mkdir(parents=True, exist_ok=True)

    src_dir = work_dir / "src"

    if url:
        # Clone from git repository
        print(f"  Cloning {url}...")
        clone_cmd = ["git", "clone"]
        if ref:
            clone_cmd.extend(["--branch", ref])
        clone_cmd.extend([url, str(src_dir)])

        result = run_cmd(clone_cmd)
        if result.returncode != 0:
            print(f"  Error cloning: {result.stderr}")
            return None

        # Checkout specific revision if specified
        if rev:
            result = run_cmd(["git", "checkout", rev], cwd=src_dir)
            if result.returncode != 0:
                print(f"  Error checking out {rev}: {result.stderr}")
                return None

        return src_dir

    elif local_dir:
        # Use a local directory (copy it to work dir)
        source_dir = local_base_path / local_dir
        if not source_dir.exists():
            print(f"  Source directory not found: {source_dir}")
            return None
        
        shutil.copytree(source_dir, src_dir)
        print(f"  Copied {source_dir} to {src_dir}")
        return src_dir

    elif lean_file_path:
        # Set up a standalone Test module from a Lean file
        source_file = get_project_root() / lean_file_path
        if not source_file.exists():
            print(f"  Source file not found: {source_file}")
            return None
        
        # Create src directory and copy the lean file as Test.lean
        src_dir.mkdir(parents=True, exist_ok=True)
        dest_file = src_dir / "Test.lean"
        shutil.copy(source_file, dest_file)
        print(f"  Copied {source_file} to {dest_file}")
        
        # Copy lean-toolchain from tests/ directory to src directory (for consistency with url flow)
        tests_toolchain = get_project_root() / "tests" / "lean-toolchain"
        dest_toolchain = src_dir / "lean-toolchain"
        if tests_toolchain.exists():
            shutil.copy(tests_toolchain, dest_toolchain)
            print(f"  Copied lean-toolchain from tests/ to src directory")
        else:
            print(f"  Warning: No lean-toolchain file found in tests/ directory")

        # Create a trivial lakefile in the src directory
        lakefile_content = '''name = "test"

[[lean_lib]]
name = "Test"'''
        lakefile_path = src_dir / "lakefile.toml"
        with open(lakefile_path, "w") as f:
            f.write(lakefile_content)
        print(f"  Created trivial lakefile")
        
        return src_dir

    else:
        # Empty directory
        src_dir.mkdir(parents=True, exist_ok=True)
        return src_dir


# =============================================================================
# build-test command
# =============================================================================


def create_test(test: dict, output_dir: Path) -> bool:
    """Create a single test."""
    name = test["name"]
    module = test.get("module")
    run_cmd_str = test.get("run")
    file_path = test.get("file")
    lean_file_path = test.get("leanfile")
    export_decls = test.get("export-decls")
    pre_build = test.get("pre-build")
    multiple = test.get("multiple", False)

    # Determine test type based on fields present
    if module:
        test_type = "module"
    elif run_cmd_str:
        test_type = "run"
    elif lean_file_path:
        test_type = "leanfile"
    elif file_path:
        test_type = "file"
    else:
        print(f"  Error: Test {name} must have 'module', 'run', 'leanfile', or 'file' field")
        return False

    # Validate multiple flag
    if multiple and test_type != "run":
        print(f"  Error: Test {name} uses 'multiple' flag but this is only valid with 'run' field")
        return False

    print(f"Creating test: {name} (type: {test_type}{'multiple' if multiple else ''})")
    output_dir.mkdir(parents=True, exist_ok=True)

    if multiple:
        # For multiple tests, output directory is testname/ instead of testname.ndjson
        final_output_dir = output_dir / name
        tmp_output_dir = output_dir / f"{name}.tmp"
    else:
        # Regular single test
        output_file = output_dir / f"{name}.ndjson"
        tmp_file = output_dir / f"{name}.ndjson.tmp"

    # Handle static file case (no work directory needed)
    if file_path:
        if multiple:
            print(f"  Error: Test {name} cannot use 'multiple' flag with static file")
            return False
        source_file = get_project_root() / file_path
        if not source_file.exists():
            print(f"  Source file not found: {source_file}")
            return False
        shutil.copy(source_file, tmp_file)
        tmp_file.rename(output_file)
        print(f"  Copied {source_file} to {output_file}")
        return True

    # These test types require lean4export
    lean4export_dir = None
    if module or lean_file_path:
        # First set up the work directory to get access to toolchain information
        work_dir = setup_source_directory(test, output_dir / "work")
        if work_dir is None:
            return False
        
        # Get the toolchain from the src directory (consistent for both leanfile and module tests)
        toolchain = get_lean_toolchain(work_dir)
        
        if not toolchain:
            print(f"  Error: No lean-toolchain found in {work_dir}")
            return False
        
        # Set up lean4export for this specific toolchain
        lean4export_dir = setup_lean4export(toolchain)
        if lean4export_dir is None:
            return False
    else:
        # Set up work directory (url, dir, or empty) for non-lean4export tests
        work_dir = setup_source_directory(test, output_dir / "work")
        if work_dir is None:
            return False

    # Run pre-build command if specified
    if pre_build:
        print(f"  Running pre-build: {pre_build}")
        result = run_cmd(pre_build, cwd=work_dir, shell=True)
        if result.returncode != 0:
            print(f"  Pre-build failed: {result.stderr}")
            return False

    # Execute based on test type
    if module or lean_file_path:
        # Both module and leanfile variants use lean4export workflow
        # leanfile is treated like module with hardcoded module name "Test"
        build_dir = work_dir
        
        # Determine module name
        if lean_file_path:
            module_name = "Test"
        else:
            module_name = module
        
        # Build the module
        print(f"  Building module {module_name}...")
        result = run_cmd(f"lake build {module_name}", cwd=build_dir, shell=True)
        if result.returncode != 0:
            print(f"  Build failed: {result.stderr}")
            return False

        # Export using lean4export
        if export_decls and not isinstance(export_decls, list):
            print(f"  Error: export-decls must be a list of strings")
            return False
        
        if export_decls:
            decls_str = ", ".join(export_decls)
            print(f"  Exporting module {module_name} ({decls_str})...")
        else:
            print(f"  Exporting module {module_name} ...")
        
        if not run_lean4export(lean4export_dir, module_name, export_decls, cwd=build_dir, out_file=tmp_file):
            return False

    elif run_cmd_str:
        # Run the script with $OUT environment variable
        print(f"  Running: {run_cmd_str}")
        env = os.environ.copy()
        
        if multiple:
            # For multiple tests, $OUT points to a temporary directory
            tmp_output_dir.mkdir(parents=True, exist_ok=True)
            env["OUT"] = str(tmp_output_dir)
        else:
            # For single tests, $OUT points to the output file
            env["OUT"] = str(tmp_file)

        result = run_cmd(run_cmd_str, cwd=work_dir, shell=True, env=env)
        if result.returncode != 0:
            print(f"  Script failed: {result.stderr}")
            return False

    if multiple:
        # For multiple tests, validate directory structure
        if not tmp_output_dir.exists():
            print(f"  Error: Script did not create output directory {tmp_output_dir}")
            return False
            
        # Check that we have good/ and/or bad/ subdirectories with .ndjson files
        good_dir = tmp_output_dir / "good"
        bad_dir = tmp_output_dir / "bad"
        
        subtests_found = []
        if good_dir.exists():
            # Sort files alphabetically to avoid dependency on filesystem order
            for ndjson_file in sorted(good_dir.glob("*.ndjson")):
                subtest_name = ndjson_file.stem
                subtests_found.append((subtest_name, "good"))
        
        if bad_dir.exists():
            # Sort files alphabetically to avoid dependency on filesystem order
            for ndjson_file in sorted(bad_dir.glob("*.ndjson")):
                subtest_name = ndjson_file.stem
                subtests_found.append((subtest_name, "bad"))
        
        if not subtests_found:
            print(f"  Error: No .ndjson files found in {tmp_output_dir}/good/ or {tmp_output_dir}/bad/")
            return False
        
        # Generate stats for each subtest
        for subtest_name, outcome in subtests_found:
            subtest_file = tmp_output_dir / outcome / f"{subtest_name}.ndjson"
            
            # Gather stats about the subtest file
            file_size = subtest_file.stat().st_size
            with open(subtest_file, "r") as f:
                line_count = sum(1 for _ in f)
            
            # Format file size and line count
            size_str = format_memory(file_size)
            lines_str = format_unitless(line_count)
                
            # Write stats JSON file
            stats_file = tmp_output_dir / outcome / f"{subtest_name}.stats.json"
            stats = {
                "name": f"{name}/{subtest_name}",
                "outcome": "accept" if outcome == "good" else "reject",
                "size": file_size,
                "size_str": size_str,
                "lines": line_count,
                "lines_str": lines_str,
                "yaml_file": f"tests/{name}.yaml",
            }
            
            # Generate and store source links from parent test
            build_info = get_build_metadata()
            source_links = generate_source_links(test, "tests", build_info.get("git_revision"))
            stats.update(source_links)
            
            # Check for subtest-name.info.json file with description
            info_file = tmp_output_dir / outcome / f"{subtest_name}.info.json"
            if info_file.exists():
                try:
                    with open(info_file, "r") as f:
                        info_data = json.load(f)
                    if "description" in info_data:
                        stats["description"] = info_data["description"]
                except Exception as e:
                    print(f"  Warning: Could not read {info_file}: {e}")
            
            with open(stats_file, "w") as f:
                json.dump(stats, f, indent=2)
        
        # Move to final location after writing statistics
        if final_output_dir.exists():
            shutil.rmtree(final_output_dir)
        tmp_output_dir.rename(final_output_dir)
        
        print(f"  Created {len(subtests_found)} subtests in {final_output_dir}")
        return True
    
    else:
        # Single test: move tmp file to final location and gather stats
        tmp_file.rename(output_file)

        # Gather stats about the created file
        file_size = output_file.stat().st_size
        with open(output_file, "r") as f:
            line_count = sum(1 for _ in f)
        
        # Format file size and line count
        size_str = format_memory(file_size)
        lines_str = format_unitless(line_count)

        print(f"  Created {output_file} ({size_str}, {lines_str} lines)")

        # Write stats JSON file
        stats_file = output_dir / f"{name}.stats.json"
        stats = {
            "name": name,
            "size": file_size,
            "size_str": size_str,
            "lines": line_count,
            "lines_str": lines_str,
            "yaml_file": f"tests/{name}.yaml",
            "outcome": test.get("outcome"),
        }
        # Add description from YAML if present
        if test.get("description"):
            stats["description"] = test["description"]
        
        # Add large field if present
        if test.get("large"):
            stats["large"] = test["large"]
            
        # Generate and store source links
        build_info = get_build_metadata()
        source_links = generate_source_links(test, "tests", build_info.get("git_revision"))
        stats.update(source_links)
        
        with open(stats_file, "w") as f:
            json.dump(stats, f, indent=2)

        return True


def cmd_build_test(args: argparse.Namespace) -> int:
    """Handle the build-test command."""
    output_dir = get_project_root() / "_build" / "tests"

    if args.name:
        # For building, we need to find from base tests, not expanded tests
        base_tests = load_test_descriptions()
        # Use pattern matching on test descriptions
        if any(char in args.name for char in ['*', '?', '[', ']']):
            tests = [test for test in base_tests if fnmatch.fnmatch(test["name"], args.name)]
        else:
            tests = [test for test in base_tests if test["name"] == args.name]
        
        if not tests:
            print(f"No tests found matching pattern: {args.name}")
            return 1
    else:
        tests = load_test_descriptions()

    # Filter out large tests if --no-large flag is set
    if args.no_large:
        original_count = len(tests)
        tests = [test for test in tests if not test.get("large", False)]
        skipped_count = original_count - len(tests)
        if skipped_count > 0:
            print(f"Skipping {skipped_count} large test(s) due to --no-large flag")

    if not tests:
        print("No tests found.")
        return 0

    success = 0
    failed = 0
    for test in tests:
        if create_test(test, output_dir):
            success += 1
        else:
            failed += 1

    print(f"\nResults: {success} succeeded, {failed} failed")
    return 0 if failed == 0 else 1


# =============================================================================
# build-checker command
# =============================================================================


def build_checker(checker: dict, build_dir: Path) -> bool:
    """Build a single checker."""
    name = checker["name"]
    version = checker.get("version", "unknown")
    build_cmd = checker.get("build")

    print(f"Building checker: {name} (version: {version})")

    # Set up source directory (for checkers, local dirs are relative to checkers/ subfolder)
    local_base_path = get_project_root() / "checkers" if checker.get("dir") else get_project_root()
    work_dir = setup_source_directory(checker, build_dir, local_base_path)
    if work_dir is None:
        return False

    # Determine the actual working directory for build commands
    if checker.get("url") or checker.get("dir"):
        # Git repos and local dirs are copied to src/ subdirectory
        actual_work_dir = work_dir
    else:
        # Empty directory case - use the checker directory directly
        actual_work_dir = build_dir / name

    # Run build command if specified
    if build_cmd:
        # Format the build command for display
        if '\n' in build_cmd:
            # Multi-line command: indent each line
            lines = build_cmd.strip().split('\n')
            formatted_cmd = '\n    '.join(lines)
            print(f"  Building:\n    {formatted_cmd}")
        else:
            # Single-line command
            print(f"  Building: {build_cmd}")
        
        result = run_cmd(build_cmd, cwd=actual_work_dir, shell=True)
        if result.returncode != 0:
            print(f"  Build failed: {result.stderr}")
            return False

    print(f"  Checker {name} built successfully")
    return True


def cmd_build_checker(args: argparse.Namespace) -> int:
    """Handle the build-checker command."""
    build_dir = get_project_root() / "_build" / "checkers"

    if args.name:
        checkers = find_items_by_pattern(args.name, "checkers")
        if not checkers:
            print(f"No checkers found matching pattern: {args.name}")
            return 1
    else:
        checkers = load_checkers()

    if not checkers:
        print("No checkers found.")
        return 0

    success = 0
    failed = 0
    for checker in checkers:
        if build_checker(checker, build_dir):
            success += 1
        else:
            failed += 1

    print(f"\nResults: {success} succeeded, {failed} failed")
    return 0 if failed == 0 else 1


# =============================================================================
# run-checker command
# =============================================================================


def run_checker_on_test(checker: dict, test: dict, build_dir: Path, tests_dir: Path, results_dir: Path) -> dict:
    """Run a checker on a test and return the result."""
    checker_name = checker["name"]
    test_name = test["name"]
    checker_run_cmd = checker["run"]

    # Use the file path stored in the test dict
    test_file = test["file"]
        
    if not test_file.exists():
        result_data = {
            "checker": checker_name,
            "test": test_name,
            "status": "error",
            "message": f"Test file not found: {test_file}",
            "exit_code": -1,
            "wall_time": 0,
            "cpu_time": 0,
            "max_rss": 0,
            "instructions": 0,
            "stdout": "",
            "stderr": "",
        }
        # Write result to JSON file (replace "/" with "_" for valid filename)
        results_dir.mkdir(parents=True, exist_ok=True)
        safe_test_name = test_name.replace("/", "_")
        result_file = results_dir / f"{checker_name}_{safe_test_name}.json"
        with open(result_file, "w") as f:
            json.dump(result_data, f, indent=2)
        return result_data

    # Determine working directory
    checker_dir = build_dir / checker_name
    if checker.get("url") or checker.get("dir"):
        work_dir = checker_dir / "src"
    else:
        work_dir = checker_dir

    work_dir.mkdir(parents=True, exist_ok=True)

    # Run the checker and track detailed performance metrics
    # Set up environment with IN variable pointing to test file
    env = os.environ.copy()
    env["IN"] = str(test_file)
    
    result = run_cmd(checker_run_cmd, cwd=work_dir, shell=True, env=env, measure_perf=True)

    exit_code = result.returncode
    if exit_code == 0:
        status = "accepted"
    elif exit_code == 1:
        status = "rejected"
    elif exit_code == 2:
        status = "declined"
    else:
        status = "error"

    result_data = {
        "checker": checker_name,
        "test": test_name,
        "status": status,
        "exit_code": exit_code,
        "wall_time": result.wall_time,
        "cpu_time": result.cpu_time,
        "max_rss": result.max_rss,
        "instructions": getattr(result, 'instructions', 0),
        "stdout": result.stdout,
        "stderr": result.stderr,
    }

    # Write result to JSON file (replace "/" with "_" for valid filename)
    results_dir.mkdir(parents=True, exist_ok=True)
    safe_test_name = test_name.replace("/", "_")
    result_file = results_dir / f"{checker_name}_{safe_test_name}.json"
    with open(result_file, "w") as f:
        json.dump(result_data, f, indent=2)

    return result_data


def cmd_run_checker(args: argparse.Namespace) -> int:
    """Handle the run-checker command."""
    build_dir = get_project_root() / "_build" / "checkers"
    tests_dir = get_project_root() / "_build" / "tests"
    results_dir = get_project_root() / "_results"

    # Determine which checkers to run
    if args.checker:
        checkers = find_items_by_pattern(args.checker, "checkers")
        if not checkers:
            print(f"No checkers found matching pattern: {args.checker}")
            return 1
    else:
        checkers = load_checkers()
        # Filter out checkers that weren't built (no build directory exists)
        built_checkers = []
        skipped_checker_names = []
        for checker in checkers:
            checker_dir = build_dir / checker["name"]
            if checker_dir.exists():
                built_checkers.append(checker)
            else:
                skipped_checker_names.append(checker["name"])
        checkers = built_checkers
        if skipped_checker_names:
            print(f"Skipping {len(skipped_checker_names)} checker(s) that weren't built: {', '.join(skipped_checker_names)}")

    # Determine which tests to run
    if args.test:
        tests = find_items_by_pattern(args.test, "tests")
        if not tests:
            print(f"No tests found matching pattern: {args.test}")
            return 1
    else:
        # Load all built tests
        tests = load_tests()

    if not checkers:
        print("No built checkers found.")
        return 0

    if not tests:
        print("No built tests found.")
        return 0

    # Sort tests by line count for consistent processing order
    tests = sort_tests_by_line_count(tests)

    results = []
    for checker in checkers:
        for test in tests:
            print(f"Running {checker['name']} on {test['name']}...", end="\n" if VERBOSE else " ", flush=True)
            result = run_checker_on_test(checker, test, build_dir, tests_dir, results_dir)
            results.append(result)
            print(f"[{result['status']}, {format_duration(result['wall_time'])}]", flush=True)

    # Summary
    print("\n" + "=" * 60)
    print("Summary:")
    print("=" * 60)

    status_counts = {"accepted": 0, "rejected": 0, "declined": 0, "error": 0}
    for r in results:
        status_counts[r["status"]] = status_counts.get(r["status"], 0) + 1

    for status, count in status_counts.items():
        if count > 0:
            print(f"  {status}: {count}")

    return 0


# =============================================================================
# build-site command
# =============================================================================


def load_results() -> dict:
    """Load all result JSON files from _results directory.
    
    Returns a dict keyed by (checker_name, test_name) tuples.
    """
    results = {}
    results_dir = get_project_root() / "_results"
    if not results_dir.exists():
        return results
    
    # Sort files alphabetically to avoid dependency on filesystem order
    for file in sorted(results_dir.glob("*.json")):
        with open(file, "r") as f:
            data = json.load(f)
            key = (data["checker"], data["test"])
            results[key] = data
    
    return results


def load_tests() -> list[dict]:
    """Load all built tests by recursively finding .stats.json files.
    
    Returns a list of test dictionaries with all data from the stats files.
    Only returns tests that have been successfully built.
    """
    tests = []
    build_tests_dir = get_project_root() / "_build" / "tests"
    
    if not build_tests_dir.exists():
        return tests
    
    # Recursively find all .stats.json files, sorted alphabetically
    for stats_file in sorted(build_tests_dir.rglob("*.stats.json")):
        try:
            with open(stats_file, "r") as f:
                test_data = json.load(f)
                
            # Determine the corresponding .ndjson file path based on stats file location
            ndjson_file = stats_file.parent / (stats_file.stem.replace('.stats', '') + '.ndjson')
            if ndjson_file.exists():
                # Add the file path that callers expect
                test_data["file"] = ndjson_file
                tests.append(test_data)
            else:
                print(f"Warning: No corresponding .ndjson file for {stats_file}")
                
        except Exception as e:
            print(f"Warning: Could not read stats file {stats_file}: {e}")
    
    return tests


def sort_tests_by_line_count(tests: list[dict]) -> list[dict]:
    """Sort tests by line count in ascending order.
    
    Args:
        tests: List of test dictionaries with line count data
        
    Returns:
        Sorted list of tests (ascending by line count)
    """
    def get_line_count(test):
        return test.get("lines", 0)
    
    return sorted(tests, key=get_line_count)


def compute_checker_stats(checker: dict, tests: list[dict], results: dict) -> dict:
    """Compute statistics for a checker across all tests.
    
    Returns a dict with:
    - accept_correct: number of tests with outcome=accept that checker accepted
    - accept_total: number of tests with outcome=accept that weren't declined
    - reject_correct: number of tests with outcome=reject that checker rejected
    - reject_total: number of tests with outcome=reject that weren't declined
    - declined_count: number of tests that checker declined
    - mathlib_time: wall time for the mathlib test (or None)
    - mathlib_cpu_time: CPU time for the mathlib test (or None)
    - mathlib_max_rss: Max RSS for the mathlib test (or None)
    - mathlib_instructions: instruction count for the mathlib test (or None)
    """
    checker_name = checker["name"]
    
    accept_correct = 0
    accept_total = 0
    reject_correct = 0
    reject_total = 0
    declined_count = 0
    mathlib_time = None
    mathlib_cpu_time = None
    mathlib_max_rss = None
    mathlib_instructions = 0
    
    for test in tests:
        test_name = test["name"]
        expected_outcome = test.get("outcome")
        
        key = (checker_name, test_name)
        result = results.get(key)
        
        if result is None:
            continue
        
        status = result.get("status")
        
        # Track mathlib performance metrics only if the test was accepted
        if test_name == "mathlib" and status == "accepted":
            mathlib_time = result.get("wall_time")
            mathlib_cpu_time = result.get("cpu_time")
            mathlib_max_rss = result.get("max_rss")
            mathlib_instructions = result.get("instructions", 0) or 0
        
        # Count declined tests
        if status == "declined":
            declined_count += 1
            continue
        
        if expected_outcome == "accept":
            accept_total += 1
            if status == "accepted":
                accept_correct += 1
        elif expected_outcome == "reject":
            reject_total += 1
            if status == "rejected":
                reject_correct += 1
    
    return {
        "accept_correct": accept_correct,
        "accept_total": accept_total,
        "reject_correct": reject_correct,
        "reject_total": reject_total,
        "declined_count": declined_count,
        "mathlib_time": mathlib_time,
        "mathlib_cpu_time": mathlib_cpu_time,
        "mathlib_max_rss": mathlib_max_rss,
        "mathlib_instructions": mathlib_instructions,
    }


def get_build_metadata() -> dict:
    """Get build metadata including timestamp, git revision, and GitHub action info."""
    metadata = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"),
        "git_revision": None,
        "git_revision_short": None,
        "github_url": None,
        "github_action_url": None,
    }
    
    # Get git revision
    try:
        result = run_cmd(["git", "rev-parse", "HEAD"], capture_output=True)
        if result.returncode == 0:
            git_revision = result.stdout.strip()
            metadata["git_revision"] = git_revision
            metadata["git_revision_short"] = git_revision[:8]
            # Build GitHub URL (assuming GitHub)
            try:
                remote_result = run_cmd(["git", "remote", "get-url", "origin"], capture_output=True)
                if remote_result.returncode == 0:
                    remote_url = remote_result.stdout.strip()
                    # Convert git URL to GitHub web URL
                    if "github.com" in remote_url:
                        if remote_url.startswith("git@"):
                            # Convert git@github.com:user/repo.git to https://github.com/user/repo
                            repo_path = remote_url.split(":")[-1].replace(".git", "")
                            metadata["github_url"] = f"https://github.com/{repo_path}/commit/{git_revision}"
                        elif remote_url.startswith("https://"):
                            repo_path = remote_url.replace("https://github.com/", "").replace(".git", "")
                            metadata["github_url"] = f"https://github.com/{repo_path}/commit/{git_revision}"
            except:
                pass
    except:
        pass
    
    # Get GitHub Action info from environment variables
    github_server = os.environ.get("GITHUB_SERVER_URL")
    github_repo = os.environ.get("GITHUB_REPOSITORY")
    github_run_id = os.environ.get("GITHUB_RUN_ID")
    
    if github_server and github_repo and github_run_id:
        metadata["github_run_id"] = github_run_id
        metadata["github_action_url"] = f"{github_server}/{github_repo}/actions/runs/{github_run_id}"
    
    return metadata


def generate_source_links(config: dict, config_type: str, git_revision: str | None = None) -> dict:
    """Generate Declaration and Source links for a test or checker configuration.
    
    Args:
        config: Test or checker configuration dict
        config_type: "tests" or "checkers" 
        git_revision: Git revision for GitHub links
        
    Returns dict with:
        declaration_url: Link to the YAML file in GitHub
        source_url: Link to the source (either URL or local dir in GitHub)
    """
    links = {
        "declaration_url": None,
        "source_url": None,
    }
    
    if not git_revision:
        return links
    
    # Generate declaration URL (YAML file in GitHub)
    base_github_url = "https://github.com/leanprover/lean-kernel-arena"
    declaration_path = f"{config_type}/{config['name']}.yaml"
    links["declaration_url"] = f"{base_github_url}/blob/{git_revision}/{declaration_path}"
    
    # Generate source URL
    url = config.get("url")
    local_dir = config.get("dir")
    leanfile = config.get("leanfile")
    rev = config.get("rev")
    
    if url:
        # External repository - check if it's a GitHub URL and we have a rev
        if rev and "github.com" in url:
            # Convert repository URL to tree/commit URL
            if url.endswith(".git"):
                repo_url = url[:-4]  # Remove .git suffix
            else:
                repo_url = url
            links["source_url"] = f"{repo_url}/tree/{rev}"
        else:
            # Use the repository URL as-is
            links["source_url"] = url
    elif local_dir:
        # Local directory in this repository
        if config_type == "checkers":
            source_path = f"checkers/{local_dir}"
        else:
            source_path = local_dir
        links["source_url"] = f"{base_github_url}/tree/{git_revision}/{source_path}"
    elif leanfile:
        # Lean file in this repository
        links["source_url"] = f"{base_github_url}/blob/{git_revision}/{leanfile}"
    
    return links


def create_test_tarball(tests: list, output_dir: Path) -> dict:
    """Create a tarball containing test files, organized by expected outcome.
    
    Returns dict with tarball_size (in bytes), good_count, and bad_count.
    """
    import tarfile
    import os
    
    tarball_path = output_dir / "lean-arena-tests.tar.gz"
    
    good_count = 0
    bad_count = 0
    
    with tarfile.open(tarball_path, "w:gz") as tar:
        for test in tests:
            # Skip large tests (by flag or by size > 1GB)
            if test.get("large", False) or test.get("size", 0) > 1024*1024*1024:
                continue
            
            # Use the file path from test data
            test_file = test["file"]
            if not test_file.exists():
                continue
                
            outcome = test.get("outcome", "unknown")
            if outcome == "accept":
                subdir = "good"
                good_count += 1
            else:
                subdir = "bad"
                bad_count += 1
            
            # Add file to tarball with appropriate subdirectory
            arcname = f"{subdir}/{test['name']}.ndjson"
            tar.add(test_file, arcname=arcname)
    
    # Get tarball size
    tarball_size = tarball_path.stat().st_size if tarball_path.exists() else 0
    
    return {
        "tarball_size": tarball_size,
        "good_count": good_count,
        "bad_count": bad_count
    }


def cmd_build_site(args: argparse.Namespace) -> int:
    """Handle the build-site command."""
    output_dir = Path(args.outdir)
    output_dir.mkdir(parents=True, exist_ok=True)

    templates_dir = get_project_root() / "templates"
    if not templates_dir.exists():
        print(f"Templates directory not found: {templates_dir}")
        return 1

    env = Environment(
        loader=FileSystemLoader(templates_dir),
        autoescape=select_autoescape(),
    )

    checkers = load_checkers()
    results = load_results()
    tests = load_tests()
    
    # Calculate global instructions per second from results with both cpu_time and instructions
    total_instructions = 0
    total_cpu_time = 0
    instruction_samples = 0
    
    for result in results.values():
        cpu_time = result.get("cpu_time", 0)
        instructions = result.get("instructions", 0)
        if cpu_time > 0 and instructions > 0:
            total_instructions += instructions
            total_cpu_time += cpu_time
            instruction_samples += 1
    
    # Calculate average instructions per second
    if total_cpu_time > 0 and instruction_samples > 0:
        instructions_per_second = total_instructions / total_cpu_time
    else:
        instructions_per_second = 0
    
    # Compute stats for each checker
    for checker in checkers:
        checker["stats"] = compute_checker_stats(checker, tests, results)

    # Sort checkers by the specified criteria:
    # 1. Number of bad tests not rejected (ascending - fewer mistakes is better)
    # 2. Number of good tests accepted (descending - more is good)  
    # 3. Number of good tests not accepted (ascending - fewer mistakes is better) 
    # 4. Number of tests declined (ascending - fewer declines is better)
    # 5. Instruction count for processing mathlib (ascending, with None/0 values last)
    def sort_key(checker):
        stats = checker["stats"]
        bad_not_rejected = stats["reject_total"] - stats["reject_correct"]  # Should be low
        good_accepted = stats["accept_correct"]  # Should be high
        good_not_accepted = stats["accept_total"] - stats["accept_correct"]  # Should be low
        declined_count = stats["declined_count"]  # Should be low
        mathlib_instructions = stats["mathlib_instructions"]
        
        # For mathlib_instructions: None/0 values should be treated as infinity (sort last)
        instructions_sort_key = mathlib_instructions if mathlib_instructions and mathlib_instructions > 0 else float('inf')
        
        # Note: For descending sort on good_accepted, we negate it
        return (bad_not_rejected, -good_accepted, good_not_accepted, declined_count, instructions_sort_key)
    
    checkers.sort(key=sort_key)

    # Get build metadata
    build_info = get_build_metadata()

    # Create test tarball
    tarball_info = create_test_tarball(tests, output_dir)

    # Build context data
    data = {
        "tests": tests,
        "checkers": checkers,
        "format_duration": format_duration,
        "format_memory": format_memory,
        "format_instructions": format_instructions,
        "format_unitless": format_unitless,
        "convert_instructions_to_time": convert_instructions_to_time,
        "instructions_per_second": instructions_per_second,
        "build_info": build_info,
        "tarball_info": tarball_info,
    }

    # Render index.html
    try:
        template = env.get_template("index.html")
        output_file = output_dir / "index.html"
        template.stream(data).dump(str(output_file))
        print(f"Generated: {output_file}")
    except Exception as e:
        print(f"Error rendering template: {e}")
        return 1

    # Generate per-checker pages
    try:
        checker_template = env.get_template("checker.html")
        for checker in checkers:
            checker_dir = output_dir / "checker" / checker["name"]
            checker_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate checker links
            checker_links = generate_source_links(checker, "checkers", build_info.get("git_revision"))
            
            # Gather results for this checker
            checker_results = []
            for test in tests:
                key = (checker["name"], test["name"])
                if key in results:
                    result = results[key].copy()
                    result["expected"] = test.get("outcome")
                    # Add test stats (test object contains all stats data)
                    result["test_stats"] = {k: v for k, v in test.items() if k not in ["name", "file"]}
                    # Add test description from stats (rendered from markdown)
                    stats_description = test.get("description", "")
                    result["test_description"] = render_markdown(stats_description)
                    # Test links are already stored in test object
                    result["test_links"] = {
                        "declaration_url": test.get("declaration_url"),
                        "source_url": test.get("source_url")
                    }
                    
                    # Add official checker results for comparison (if available)
                    official_key = ("official", test["name"])
                    official_result = results.get(official_key)
                    result["official"] = official_result
                    
                    checker_results.append(result)
            
            # Sort checker results by name (alphabetical order)
            checker_results.sort(key=lambda result: result.get("test", ""))
            
            # Create a copy of checker data with rendered description
            checker_with_rendered_desc = checker.copy()
            checker_with_rendered_desc["description"] = render_markdown(checker.get("description", ""))
            
            checker_data = {
                "checker": checker_with_rendered_desc,
                "checker_links": checker_links,
                "results": checker_results,
                "format_duration": format_duration,
                "format_memory": format_memory,
                "format_instructions": format_instructions,
                "format_unitless": format_unitless,
                "convert_instructions_to_time": convert_instructions_to_time,
                "instructions_per_second": instructions_per_second,
                "build_info": build_info,
            }
            
            output_file = checker_dir / "index.html"
            checker_template.stream(checker_data).dump(str(output_file))
            print(f"Generated: {output_file}")
    except Exception as e:
        print(f"Error rendering checker template: {e}")
        return 1

    # Copy static files if they exist
    static_dir = templates_dir / "static"
    if static_dir.exists():
        shutil.copytree(static_dir, output_dir / "static", dirs_exist_ok=True)
        print(f"Copied static files to: {output_dir / 'static'}")

    print(f"\nSite built successfully in: {output_dir}")
    return 0


# =============================================================================
# Main entry point
# =============================================================================


def main() -> int:
    """Main entry point."""
    global VERBOSE
    
    parser = argparse.ArgumentParser(
        prog="lka",
        description="Lean Kernel Arena - Tool for managing Lean kernel tests and checkers",
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Print commands being executed and their stats",
    )
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # build-test command
    build_test_parser = subparsers.add_parser(
        "build-test",
        help="Build test files from test definitions",
    )
    build_test_parser.add_argument(
        "name",
        nargs="?",
        help="Name or glob pattern of the test to build (default: all tests)",
    )
    build_test_parser.add_argument(
        "--no-large",
        action="store_true",
        help="Skip large tests (marked with 'large: true' in YAML)",
    )

    # build-checker command
    build_checker_parser = subparsers.add_parser(
        "build-checker",
        help="Build checkers from checker definitions",
    )
    build_checker_parser.add_argument(
        "name",
        nargs="?",
        help="Name or glob pattern of the checker to build (default: all checkers)",
    )

    # run-checker command
    run_checker_parser = subparsers.add_parser(
        "run-checker",
        help="Run checkers on tests",
    )
    run_checker_parser.add_argument(
        "--checker",
        help="Name or glob pattern of the checker to run (default: all checkers)",
    )
    run_checker_parser.add_argument(
        "--test",
        help="Name or glob pattern of the test to run (default: all tests)",
    )

    # build-site command
    build_site_parser = subparsers.add_parser(
        "build-site",
        help="Build the website",
    )
    build_site_parser.add_argument(
        "--outdir",
        default="_out",
        help="Output directory for the website (default: _out)",
    )

    args = parser.parse_args()
    
    # Set global verbose flag
    VERBOSE = args.verbose

    if args.command is None:
        parser.print_help()
        return 0

    if args.command == "build-test":
        return cmd_build_test(args)
    elif args.command == "build-checker":
        return cmd_build_checker(args)
    elif args.command == "run-checker":
        return cmd_run_checker(args)
    elif args.command == "build-site":
        return cmd_build_site(args)
    else:
        parser.print_help()
        return 1


if __name__ == "__main__":
    sys.exit(main())
